\documentclass[conference]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{array}
\usepackage{makecell}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125em}}

\begin{document}
    
    \title{Comparison of Image Preprocessing Techniques for Vehicle License Plate Recognition Using OCR: Performance and Accuracy Evaluation}
    
    \author{\IEEEauthorblockN{1\textsuperscript{st} Renato Augusto Tavares}
        \IEEEauthorblockA{\textit{Institute of Informatics} \\
            \textit{Federal University of Goiás} \\
            Goiânia, Goiás \\
            rat@discente.ufg.br}
        
        \and
        \IEEEauthorblockN{2\textsuperscript{nd} Ronaldo Martins da Costa}
        \IEEEauthorblockA{\textit{Institute of Informatics} \\
            \textit{Federal University of Goiás} \\
            Goiânia, Goiás \\
            ronaldocosta@ufg.br}}
    
    \maketitle
    
    \begin{abstract}
        The increasing use of Artificial Intelligence solutions has resulted in an explosion of image capture and its application in machine learning models. However, the lack of standardization in image quality generates inconsistencies in the results of these models. To mitigate this problem, Optical Character Recognition (OCR) is often used as a preprocessing technique, but it still faces challenges in scenarios with inadequate lighting, low resolution, and perspective distortions.
        
        This study aims to explore and evaluate various preprocessing techniques, such as grayscale conversion, CLAHE in RGB, and Bilateral Filter, applied to vehicle license plate recognition. Each technique is analyzed individually and in combination, using metrics such as accuracy, precision, recall, F1-score, ROC curve, AUC, and ANOVA, with the goal of identifying the most effective method. The study uses a dataset of Brazilian vehicle plates, widely used in OCR applications. The research contributes with a detailed analysis of the best preprocessing practices, offering insights to optimize OCR performance in real-world scenarios.
    \end{abstract}
    
    \begin{IEEEkeywords} 
        ocr, optical character recognition, vehicle plates, image preprocessing, artificial intelligence
    \end{IEEEkeywords}
    
    \section{Introduction}
    With the increasing use of Artificial Intelligence solutions, image capture and subsequent application in machine learning models have grown exponentially. However, the quality and technique used to capture these images do not follow a standard, resulting in inconsistent or incoherent results in AI models. To mitigate this problem, researchers often resort to Optical Character Recognition (OCR) techniques to preprocess images before using them in machine learning models, ensuring greater accuracy in the results.
    
    The initial concept of OCR was developed in the 1920s by Emanuel Goldberg, but it wasn't until the 1970s that more advanced techniques began to emerge, with contributions from Ray Kurzweil. Despite more than 50 years of technological evolution, OCR still faces significant challenges in pattern detection, particularly when conditions are not ideal. Factors such as inadequate lighting, low resolution, noise in images, and distortions caused by perspective or angle continue to hinder OCR performance. These limitations are particularly evident in real-world scenarios, such as vehicle license plate recognition, where systems need to deal with a wide variety of environmental conditions.
    
    The goal of this work is, aware of the mentioned limitations, to address research questions related to the current state of OCR and test various image preprocessing techniques, such as grayscale conversion, the use of CLAHE (Contrast Limited Adaptive Histogram Equalization) in RGB images, Bilateral Filter, among others. These techniques will be evaluated individually and combined, aiming to determine which offer the best results in terms of accuracy and efficiency in plate recognition. To ensure the validity of the results, we use statistical metrics such as accuracy, precision, recall, F1-score, ROC curve, AUC, and ANOVA, allowing a rigorous quantitative analysis of the performance of each preprocessing technique.
    
    To make the work replicable, we chose to use a dataset of Brazilian vehicle plates, a common application for OCR. Vehicle plate detection is widely explored in studies involving neural networks, as shown in recent works, including "Comparative Analysis of EasyOCR and TesseractOCR for Automatic License Plate Recognition" \cite{b1} and "Real-Time License Plate Detection and Recognition System using YOLOv7x and EasyOCR" \cite{b2}. These articles demonstrate how different OCR techniques can be applied to plate recognition in real conditions and serve as the basis for our dataset and methodology choices.
    
    Additionally, the choice to work with captures from a DSLR camera at different distances and lighting conditions allows us to test how OCR behaves in scenarios where image quality can vary significantly, as addressed in "Deep Learning Model for Automatic Number License Plate Detection and Recognition System" \cite{b3}. The use of different focal lengths is also aligned with the research on plate capture at different angles and distances, enabling a comprehensive analysis of the impact of these variations on OCR performance.
    
    Therefore, this study aims not only to identify the most effective preprocessing technique but also to provide a set of best practices for researchers working with plate recognition and other OCR applications. By combining a rigorous validation process with a realistic dataset and advanced preprocessing techniques, we hope to offer a significant contribution to the field of optical character recognition, with potential impact in traffic monitoring, vehicle security, and urban automation industries.
    
    \section{Research Methodology and Research Questions}
    Systematic literature reviews have gained increasing relevance in academic research as they follow a structured, rigorous, and reliable methodology for searching and analyzing publications. This type of review allows readers to quickly and effectively gain an overview of the field of study \cite{b4}. Used to deepen knowledge about specific areas, systematic reviews also allow the identification of gaps and opportunities for future investigations. The main goal of this approach is to locate, interpret, evaluate, and classify all relevant articles on previously defined research questions.
    
    In this study, a five-step systematic review process was adopted, aiming to map the existing literature, define research questions, and identify new relevant keywords. Figure~\ref{img1} presents the stages followed during the review, while Table~\ref{tab1} shows the number of articles analyzed throughout the process.
    
    During the initial selection, using strategically defined keywords, the \textbf{IEEE Digital Library} returned a total of \textbf{190 articles}. As part of the rigorous filtering process, we applied exclusion criteria to ensure the relevance and suitability of the studies, resulting in the elimination of 42 articles, leaving a total of 148 for analysis. In the next step, we removed duplicate articles, identified both by human error and internal search tool failures, which led to the exclusion of two more articles, leaving \textbf{146 articles} that were carefully read and analyzed for this work.
    
    \begin{table}[H]
        \caption{Stages of Systematic Literature Review}
        \begin{center}
            \begin{tabular}{l|c|c|c|c|}
                \cline{2-5}
                & Start & Step 1 & Step 2 & Step 3 \\ \hline
                \multicolumn{1}{|l|}{Initial Selection}       & 190    &         &         &         \\ \hline
                \multicolumn{1}{|l|}{Exclusion Criteria}      &        & -42     &         &         \\ \hline
                \multicolumn{1}{|l|}{Duplicate Removal}       &        &         & -2      &         \\ \hline
                \multicolumn{1}{|l|}{Final Result}            &        &         &         & 146     \\ \hline
            \end{tabular}
            \label{tab1}
        \end{center}
    \end{table}
    
    This systematic survey was conducted with great care to ensure that we were not only informed about the state of the art in the area of optical character recognition (OCR) and image processing but also that we could accurately identify the main open research questions. Each stage of the process was designed to provide a comprehensive view of current trends and challenges in the field, ensuring that our research aligned with the latest scientific developments.
    
    Through this approach, we were able to not only map the most relevant contributions but also identify gaps in the literature that allowed us to formulate a work that truly contributes to the advancement of the area. The careful attention to ensuring that the articles were appropriate to our study theme was essential in ensuring that we addressed the key research questions in an innovative and rigorous way, consolidating our work on a solid scientific foundation.
    
    \begin{figure*}[htbp]
        \centerline{\includegraphics[width=\textwidth]{img1.png}}
        \caption{Stages of the systematic literature review.}
        \label{img1}
    \end{figure*}
    
    The research questions were designed to precisely define the purpose of the study, considering its scope. The research questions guiding this work are as follows:
    
    \begin{itemize}
        \item \textbf{RQ1:} How is character recognition of vehicle plates done in the current state of the art?
        \item \textbf{RQ2:} Is there an image processing algorithm that provides 100% accuracy in plate character recognition?
        \item \textbf{RQ3:} How can the use of OCR techniques ensure precise and efficient identification of vehicle plates under different lighting conditions and capture angles?
        \item \textbf{RQ4:} What is the success rate of automated vehicle license plate recognition systems using OCR?
    \end{itemize}
    
    \section{Statistical Evaluation Metrics Used}
    
    As mentioned in the introduction, it is essential to thoroughly understand image preprocessing algorithms to significantly improve the accuracy of character recognition systems. Effective training of Artificial Intelligences (AI) and the development of advanced technologies such as autonomous vehicles, automatic license plate identification systems, and automated drone flights depend on highly accurate and reliable OCR algorithms. Any error in these systems can not only cause financial losses but also endanger lives, highlighting the importance of improving these processes.
    
    In this section, we will discuss in depth the five preprocessing algorithms applied in this work, detailing their advantages, disadvantages, and the scenarios in which each performs best, as well as those where their use is less efficient. The goal is to establish a robust statistical metrics foundation, providing a clear and well-grounded quantitative analysis that can serve as a reference for researchers and developers who want to train AIs using OCR.
    
    Among the algorithms used, we highlight techniques such as grayscale conversion and the use of CLAHE (Contrast Limited Adaptive Histogram Equalization) in RGB images, widely recognized for improving contrast and the visibility of important details, as demonstrated in previous studies on vehicle plate recognition [1]. In addition, the Bilateral Filter was tested, known for its effectiveness in noise reduction without compromising image edges, which is particularly relevant in unfavorable lighting situations or damaged plates.
    
    Previous studies, such as "Comparative Analysis of EasyOCR and TesseractOCR for Automatic License Plate Recognition" \cite{b1} and "Deep Learning Model for Automatic Number License Plate Detection and Recognition System" \cite{b2}, have already demonstrated the importance of these techniques in improving OCR accuracy, especially when applied to images of plates captured under various conditions, such as different angles, variable distances, and uneven lighting. By exploring these approaches, this study aims to determine which preprocessing technique offers the best results in terms of accuracy, precision, recall, and F1-score, as well as identifying the limitations of each method in specific situations.
    
    At the end of this analysis, we hope to offer a viable and scientifically grounded solution for researchers and engineers who need to optimize character recognition in their AI applications, whether for traffic monitoring systems, vehicle automation, or other critical scenarios where accuracy is essential for safety and efficiency.
    
    To facilitate understanding of the article, let's define some of the metrics used in the text.
    
    \subsection{Accuracy}
    
    \textbf{Accuracy} is an evaluation metric used to measure the effectiveness of a classification model. It represents the fraction of correct predictions out of the total predictions made by the model. Accuracy is an important metric in cases where classes are balanced, i.e., the number of positive and negative cases is approximately the same.
    
    The formula for calculating accuracy is:
    
    \[
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    \]
    
    Where:
    
    \begin{itemize}
        \item \textbf{TP} (True Positives) are the correct predictions for the positive class (correctly predicted plates).
        \item \textbf{TN} (True Negatives) are the correct predictions for the negative class (plates that were not incorrectly predicted).
        \item \textbf{FP} (False Positives) are incorrect predictions for the positive class (incorrectly predicted plates).
        \item \textbf{FN} (False Negatives) are incorrect predictions for the negative class (correct plates predicted incorrectly).
    \end{itemize}
    
    \subsection{Precision}
    
    \textbf{Precision} is an evaluation metric that indicates the proportion of correct positive predictions in relation to the total positive predictions made by the model. Precision is useful in situations where the cost of false positives is high, i.e., when it is important to minimize positive prediction errors.
    
    The formula for precision is:
    
    \[
    \text{Precision} = \frac{TP}{TP + FP}
    \]
    
    Where:
    
    \begin{itemize}
        \item \textbf{TP} (True Positives) are the correct predictions for the positive class (correctly predicted plates).
        \item \textbf{FP} (False Positives) are incorrect predictions for the positive class (plates predicted as correct but incorrect).
    \end{itemize}
    
    \subsection{Recall (Sensitivity)}
    
    \textbf{Recall}, also known as sensitivity or true positive rate, measures the model's ability to correctly identify positive instances (in this case, correctly predicted plates). Recall is particularly important when the goal is to minimize false negatives, i.e., when it is crucial to detect all positive cases.
    
    The formula for \textbf{Recall} is:
    
    \[
    \text{Recall} = \frac{TP}{TP + FN}
    \]
    
    Where:
    
    \begin{itemize}
        \item \textbf{TP} (True Positives) are the correct predictions for the positive class (correctly predicted plates).
        \item \textbf{FN} (False Negatives) are incorrect predictions for the negative class (plates that should have been predicted correctly but were predicted incorrectly).
    \end{itemize}
    
    In practical terms, \textbf{Recall} measures the proportion of true positives identified in relation to the total instances that truly belong to the positive class.
    
    \subsection{F1-score}
    
    The \textbf{F1-score} is a metric that combines \textbf{precision} and \textbf{recall} into a single measure. It is particularly useful when there is a balance between minimizing both false positives and false negatives. The \textbf{F1-score} is the harmonic mean between precision and recall, offering a balanced view of model performance.
    
    The formula for the \textbf{F1-score} is:
    
    \[
    F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \]
    
    Where:
    
    \begin{itemize}
        \item \textbf{Precision} measures the proportion of correct positive predictions in relation to the total positive predictions made.
        \item \textbf{Recall} measures the proportion of positive instances correctly identified.
    \end{itemize}
    
    The \textbf{F1-score} ranges from 0 to 1, where 1 indicates the best possible performance (perfect precision and recall), and 0 indicates the worst performance.
    
    \subsection{ROC Curve and AUC (Area Under the Curve)}
    
    The \textbf{ROC (Receiver Operating Characteristic) Curve} is a graphical tool used to evaluate the performance of a binary classifier. It shows the relationship between the \textbf{True Positive Rate (TPR)} and the \textbf{False Positive Rate (FPR)} as the model's decision threshold varies. The ROC curve is useful for visualizing the model's performance at different sensitivity levels.
    
    The \textbf{True Positive Rate} (or Recall) is given by:
    
    \[
    \text{TPR} = \frac{TP}{TP + FN}
    \]
    
    Where:
    
    \begin{itemize}
        \item \textbf{TP} (True Positives) are the correct predictions for the positive class.
        \item \textbf{FN} (False Negatives) are the correct positive instances that the model predicted incorrectly as negative.
    \end{itemize}
    
    The \textbf{False Positive Rate (FPR)} is given by:
    
    \[
    \text{FPR} = \frac{FP}{FP + TN}
    \]
    
    Where:
    
    \begin{itemize}
        \item \textbf{FP} (False Positives) are the incorrect predictions for the positive class.
        \item \textbf{TN} (True Negatives) are the correct predictions for the negative class.
    \end{itemize}
    
    The \textbf{AUC (Area Under the ROC Curve)} is a measure that summarizes the overall performance of the classifier. It ranges from 0 to 1, where:
    
    \begin{itemize}
        \item A value of \textbf{1.0} indicates a perfect classifier that correctly predicts all instances.
        \item A value of \textbf{0.5} indicates a classifier with no predictive power (equivalent to random guessing).
    \end{itemize}
    
    The AUC formula is not directly represented, but it is defined as the area under the ROC curve, calculated numerically.
    
    \subsection{ANOVA}
    
    \textbf{ANOVA}, which stands for \textbf{Analysis of Variance}, is a statistical test used to compare the means of more than two groups. We use ANOVA to compare the accuracy rates of different vehicle plate recognition models.
    
    The idea of ANOVA is to check if there is a significant difference between the means of different groups. The test starts with the null hypothesis ($H_0$) that all group means are equal, meaning there is no real difference between the models' accuracy rates. The alternative hypothesis ($H_1$) is that at least one mean is different, indicating a significant difference between the models' performance.
    
    To understand the mathematical formula for ANOVA calculation, specifically for one-way ANOVA, it is important to understand the following terms:
    
    \begin{enumerate}
        \item \textbf{Sum of Squares (SS)}
        \begin{itemize}
            \item \textbf{Between-Groups Sum of Squares (SSB)}: Measures variability between the group means.
            \item \textbf{Within-Groups Sum of Squares (SSW)}: Measures variability within each group.
            \item \textbf{Total Sum of Squares (SST)}: Measures total variability, considering all groups as a single sample.
        \end{itemize}
        \item \textbf{Degrees of Freedom (df)}
        \begin{itemize}
            \item \textbf{Between-Groups Degrees of Freedom (dfB)}: Number of groups minus 1: \( df_B = k - 1 \), where \( k \) is the number of groups.
            \item \textbf{Within-Groups Degrees of Freedom (dfW)}: Total number of observations minus the number of groups: \( df_W = N - k \), where \( N \) is the total number of observations.
        \end{itemize}
        \item \textbf{Mean Squares (MS)}
        \begin{itemize}
            \item \textbf{Between-Groups Mean Squares (MSB)}: Calculated by dividing SSB by its degrees of freedom.
            \item \textbf{Within-Groups Mean Squares (MSW)}: Calculated by dividing SSW by its degrees of freedom.
        \end{itemize}
    \end{enumerate}
    
    The \textbf{F value} in ANOVA is calculated as the ratio between the variability between the groups and the variability within the groups:
    
    \[
    F = \frac{MSB}{MSW}
    \]
    
    To calculate the F value, the following steps are followed:
    
    \begin{enumerate}
        \item \textbf{Calculate the Total Sum of Squares (SST)}
        \begin{itemize}
            \item SST is the sum of the variation of each value relative to the overall mean.
            
            \[
            SST = \sum_{i=1}^{N} (X_i - \bar{X})^2
            \]
            
            Where \( X_i \) are the observed values and \( \bar{X} \) is the overall mean.
        \end{itemize}
        
        \item \textbf{Calculate the Between-Groups Sum of Squares (SSB)}
        \begin{itemize}
            \item SSB measures the variation of the group means relative to the overall mean.
            
            \[
            SSB = \sum_{j=1}^{k} n_j (\bar{X}_j - \bar{X})^2
            \]
            
            Where \( n_j \) is the number of observations in group \( j \), \( \bar{X}_j \) is the group \( j \) mean, and \( \bar{X} \) is the overall mean.
        \end{itemize}
        
        \item \textbf{Calculate the Within-Groups Sum of Squares (SSW)}
        \begin{itemize}
            \item SSW measures the variation of individual values relative to their group mean.
            
            \[
            SSW = \sum_{j=1}^{k} \sum_{i=1}^{n_j} (X_{ij} - \bar{X}_j)^2
            \]
            
            Where \( X_{ij} \) are the observations in group \( j \), and \( \bar{X}_j \) is the group \( j \) mean.
        \end{itemize}
        
        \item \textbf{Calculate the Mean Squares}
        \begin{itemize}
            \item \textbf{Between-Groups Mean Squares (MSB)}
            
            \[
            MSB = \frac{SSB}{df_B} = \frac{SSB}{k - 1}
            \]
            
            \item \textbf{Within-Groups Mean Squares (MSW)}
            
            \[
            MSW = \frac{SSW}{df_W} = \frac{SSW}{N - k}
            \]
        \end{itemize}
        
        \item \textbf{Calculate the F Value}
        \begin{itemize}
            \item Finally, the F value is the ratio between \( MSB \) and \( MSW \):
            
            \[
            F = \frac{MSB}{MSW}
            \]
        \end{itemize}
    \end{enumerate}
    
    A high \textbf{F value} indicates that the variability between the group means (between-group variability) is greater than the within-group variability, suggesting a significant difference between the groups. A low \textbf{F value} suggests that within-group variability is greater or comparable to between-group variability, indicating that the group means do not differ significantly.
    
    In summary, the F statistic in ANOVA measures how different the group means are compared to within-group variability. The formula involves comparing the variability \textbf{between groups} (how the group means differ from the overall mean) with the variability \textbf{within groups} (how individual values differ from their group mean).
    
    \section{Preprocessing Algorithms Used}
    
    \subsection{No Preprocessing - EasyOCR}
    
    The first algorithm tested in this study actually did not involve any preprocessing technique. Initially, we decided to run EasyOCR directly on the raw images from the dataset, without any prior treatment, to observe how the OCR system would react to the original conditions of the images and assess the baseline accuracy of the library before applying enhancement techniques. This procedure is fundamental for establishing a baseline for OCR performance and verifying whether subsequent preprocessing techniques bring significant improvements.
    
    EasyOCR is an open-source library created by the team at Jaided AI, actively maintained by the community and its original developers. It was designed to be an efficient and easy-to-use solution for text recognition in images, supporting over 80 languages. Its source code is publicly available on GitHub, allowing researchers and software engineers to contribute to its development or adapt the library to the specific needs of their applications.
    
    One of EasyOCR's greatest advantages is its simplicity of use and flexibility to work with different datasets and image conditions. The library uses a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) for character detection and recognition, and it is highly efficient at handling printed characters in regular fonts, such as vehicle plates or documents. Additionally, EasyOCR is known for its ability to process text in different languages without the need for extensive configurations, making it a popular choice in various text recognition scenarios.
    
    However, EasyOCR also has some limitations. Its performance can be hindered when dealing with low-quality images, especially those with significant noise, poor lighting, or perspective distortions, which often occur in traffic images or outdoor environments. These deficiencies make the use of preprocessing critical to improving results in many scenarios, as explored in our study and in works like "Automatic Vehicle License Plate Recognition Using Lightweight Deep Learning Approach" \cite{b5}, which shows that lighter solutions may be suitable in certain scenarios but may require preprocessing improvements to achieve optimal results. Additionally, studies like "Cognitive Number Plate Recognition using Machine Learning and Data Visualization Techniques" \cite{b6} indicate that complementary techniques, such as data visualization, can be integrated with OCRs like EasyOCR to optimize result interpretation in large data volumes.
    
    EasyOCR is widely used in various practical application scenarios, including reading vehicle plates in traffic monitoring systems, document digitization, and even reading street signs and advertisements. These scenarios benefit from EasyOCR's ability to efficiently recognize text in images of different resolutions and formats. However, to ensure the robustness of recognition in more complex conditions, such as worn plates, reflections, or unfavorable angles, image preprocessing techniques need to be applied, as we will see in the following sections.
    
    In summary, EasyOCR serves as an effective base tool for character recognition, but its performance in real-world scenarios can be significantly improved with the use of appropriate preprocessing techniques. Its versatility and open-source nature make it an attractive choice for a wide range of applications, and its continued use and maintenance by the community ensure that it continues to evolve to meet the needs of both academic and industrial research.
    
    The first statistical metric we used was \textbf{accuracy}, which is a metric used to measure the effectiveness of a classification model. It represents the fraction of correct predictions out of the total predictions made by the model. Accuracy is an important metric when classes are balanced, meaning the number of positive and negative cases is approximately the same.
    
    In this case, \textbf{accuracy} was calculated by comparing the Correct Plate column with the Predicted Plate column. The proportion of correct predictions was approximately \textbf{71.7\%}. This means that the model was able to correctly predict about \textbf{71.7\%} of the vehicle plates.
    
    The second metric used was \textbf{precision}, which was calculated by comparing the Correct Plate column with the Predicted Plate column. The proportion of correct positive predictions was \textbf{71.7\%}, meaning that among all predictions made as positive (correctly detected plates), approximately \textbf{71.7\%} were correct.
    
    The third metric used was the \textbf{F1-score}, as both precision and \textbf{recall} were calculated as \textbf{71.7\%}. Substituting these values into the F1-score formula:
    
    \[
    F1 = 2 \times \frac{0.717 \times 0.717}{0.717 + 0.717} = 0.717
    \]
    
    Thus, the \textbf{F1-score} obtained was \textbf{71.7\%}, reflecting a balance between precision and recall. This indicates that the model has a balanced performance in terms of correctly detecting plates and minimizing errors.
    
    The fourth metric used to compare the models was the \textbf{ROC Curve}, which was generated from the comparison between correct and predicted plates. In our case, the \textbf{AUC} was \textbf{1.0}, indicating that the model was able to perfectly classify all instances without making errors.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img4.png}}
        \caption{The ROC curve was generated with an AUC (Area Under the Curve) of 1.0.}
        \label{img4}
    \end{figure}
    
    This result indicates excellent model performance, as it correctly distinguished all plates \textbf{without false positives} or \textbf{false negatives}.
    
    The arithmetic mean of the model's execution time was \textbf{7.26 seconds}, representing the average value considering all recorded execution times. The median was \textbf{6.59 seconds}, indicating the central point of the times, which may be a more representative measure if there are outliers distorting the mean.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img5.png}}
        \caption{The arithmetic mean and median of the model's execution time}
        \label{img5}
    \end{figure}
    
    In the figure~\ref{img5} above, you can see the visual comparison between the mean and median. The difference between the two values indicates that, although the average time is slightly higher, the median, which is less sensitive to outliers, shows a more typical time that the models tend to reach.
    
    Below in figure~\ref{img6} is the Gaussian distribution of the model's execution times, overlaid on the histogram of real data. The blue curve represents the theoretical Gaussian distribution based on the mean and standard deviation of the execution times. The green histogram shows how execution times are distributed in practice.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img6.png}}
        \caption{Gaussian Distribution of Execution Times}
        \label{img6}
    \end{figure}
    
    This visualization helps to identify whether execution times approximately follow a normal (Gaussian) distribution or if there are significant deviations.
    
    \subsection{Grayscale - EasyOCR}
    
    The second preprocessing algorithm used in this study was \textbf{grayscale conversion}. The goal was to evaluate how the dataset would behave under this technique and compare the \textbf{accuracy} rate with the results obtained using EasyOCR without any preprocessing. Grayscale conversion is a simple and widely used image processing technique, as it reduces the complexity of the information present in an image by eliminating colors, retaining only the luminance intensity of the pixels. However, the results indicated that for our dataset, this technique did not offer improvements.
    
    Specifically, we observed that the \textbf{accuracy} — which represents the fraction of correct predictions out of the total predictions made — dropped from \textbf{71.7\%} (without preprocessing) to \textbf{70.75\%} when we applied grayscale. This suggests that by removing color information, the model lost some ability to distinguish certain visual patterns important for character recognition. This result contrasts with other situations where grayscale may be beneficial, such as in the recognition of simple texts where color does not play a relevant role.
    
    Another indicator that suffered a slight degradation was \textbf{precision}, which dropped from \textbf{71.7\%} without preprocessing to \textbf{70.75\%} when using grayscale. Precision measures the proportion of true positives in relation to the total positive predictions made by the model. The drop, though small, indicates that the model had more difficulty making correct predictions when color information was eliminated. In other contexts, such as described in the study "A Hybrid Deep Learning Algorithm for License Plate Detection and Recognition in Vehicle-to-Vehicle Communications" \cite{b7}, grayscale can be effective when working with images in controlled environments or with uniform lighting. However, in our scenario, which involves variation in lighting conditions and capture angles, the absence of color seems to have been a negative factor.
    
    \textbf{Recall} — which corresponds to the proportion of correctly detected plates relative to the total plates in the dataset — also showed a slight drop, from \textbf{71.7\%} (without preprocessing) to \textbf{70.75\%} with grayscale. Recall is important for evaluating the model's ability to correctly detect all plate instances, and this decrease indicates that the model may have failed to capture certain characters under adverse conditions, such as worn or partially obstructed plates.
    
    The harmonic mean between precision and recall, known as the \textbf{F1-score}, was also affected, reaching \textbf{70.75\%} after applying grayscale preprocessing, compared to the \textbf{71.7\%} obtained without preprocessing. This metric is particularly useful in plate recognition scenarios, where the goal is to balance the model's ability to be precise (not generate false positives) and complete (not fail to detect plates). The negative impact of grayscale conversion in this case suggests that color is an important feature in our specific dataset, reinforcing the need to carefully assess the context of applying this technique.
    
    The \textbf{ROC curve (Receiver Operating Characteristic)} and the \textbf{AUC (Area Under the Curve)} remained unchanged from the model without preprocessing, which was expected since our dataset involves predicting only a single plate at a time without requiring probabilistic calculation. As a result, the model's behavior, both with grayscale conversion and without preprocessing, remained identical in this aspect, as observed in other related studies, such as "License Plate Recognition System Based on Improved YOLOv5 and GRU" \cite{b8}, where ROC/AUC also remained stable in scenarios with few predicted classes, as shown in Figure~\ref{img7}.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img7.png}}
        \caption{The ROC (Receiver Operating Characteristic) curve and AUC (Area Under the Curve) remained unchanged}
        \label{img7}
    \end{figure}
    
    Grayscale conversion has some important advantages. One is the simplicity and reduction of image data volume, which can speed up processing and save computational resources. In scenarios where color does not play a significant role, such as document digitization or recognition of texts in standardized fonts, grayscale can be highly efficient.
    
    On the other hand, one of the main disadvantages is that in complex scenarios, such as vehicle plate recognition under different lighting conditions, angles, and wear, the removal of color can hinder the recognition of crucial details. These challenges are especially relevant when plates have contrasting background colors and characters, which help the model better distinguish patterns.
    
    Grayscale is widely used in printed documents, where color does not influence reading, and in basic computer vision applications where the amount of visual information to be processed needs to be reduced. However, in \textbf{vehicle plate recognition systems}, especially those in dynamic environments with varying light and angles, color can be essential for OCR accuracy and reliability.
    
    On the other hand, one could argue in favor of using grayscale by claiming that the processing time for a grayscale image is theoretically faster than processing a color image. This is because grayscale images have only a single pixel intensity layer, whereas color images, typically in RGB format, have three layers (red, green, and blue), naturally increasing the amount of data to be processed. This reduction in image complexity should, therefore, result in faster performance for character recognition algorithms like EasyOCR.
    
    However, when observing the practical results, the data obtained contradicts this expectation. The average execution time of the model when processing grayscale images was approximately \textbf{8.88 seconds}, with a median of \textbf{7.01 seconds}, as shown in Figure~\ref{img8}. In contrast, when using EasyOCR with color images, the average execution time was \textbf{7.26 seconds}, while the median was \textbf{6.59 seconds}, as shown in Figure~\ref{img5}. These numbers reveal that processing color images surprisingly presented faster performance than processing grayscale images.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img8.png}}
        \caption{The arithmetic mean and median of the model's execution time}
        \label{img8}
    \end{figure}
    
    This result can be explained by several factors. First, although grayscale reduces the amount of data in an image, the conversion process to grayscale adds an extra step to the image processing pipeline. Additionally, EasyOCR is optimized to work with color images, where color information can help the model differentiate between background and characters, reducing the computational effort required to distinguish letters or numbers present in the image. As described in "Cognitive Number Plate Recognition using Machine Learning and Data Visualization Techniques" \cite{b6}, preserving color characteristics in some cases can be beneficial, even if it slightly increases the amount of data processed, as it can improve character segmentation and overall image clarity. In Figure~\ref{img9}, we show the \textbf{Gaussian Distribution of Execution Times}, and it is evident that there was no impact from outlier data.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img9.png}}
        \caption{Gaussian Distribution of Execution Times}
        \label{img9}
    \end{figure}
    
    Additionally, the execution time of an OCR model is not determined solely by the amount of visual data processed, but also by the model's ability to efficiently interpret and recognize patterns. In "Automatic Vehicle License Plate Recognition Using Lightweight Deep Learning Approach" \cite{b5}, for example, it was demonstrated that lighter models, even with fewer data to process, do not necessarily result in faster performance if the algorithm cannot clearly identify patterns. This principle also applies here, as when using color images, EasyOCR may be benefiting from the additional color information to perform more accurate and consequently faster character detection.
    
    In summary, although theoretically grayscale should reduce processing time, the data shows that EasyOCR, when used with color images, offers superior performance in terms of execution speed. This result demonstrates that the efficiency of OCR processing does not depend solely on the amount of visual data, but also on the algorithm's optimization for different types of images.
    
    \subsection{CLAHE in RGB - EasyOCR}
    
    The third preprocessing algorithm applied in this study was \textbf{CLAHE (Contrast Limited Adaptive Histogram Equalization)}, an adaptive histogram equalization technique with contrast limitation, widely used to improve contrast in images with uneven lighting. The goal of this technique is to enhance visual details, especially in areas with low luminance variation, without excessively amplifying image noise. Thus, we sought to evaluate how the dataset would behave when applying CLAHE and compare it with the results obtained using EasyOCR without any preprocessing.
    
    When observing the results, we found that \textbf{accuracy} — the fraction of correct predictions relative to the total predictions made — slightly dropped from \textbf{71.7\%} (without preprocessing) to \textbf{70.75\%} after applying CLAHE. While the technique is effective for enhancing detail visibility in some images, it seems that in our case, CLAHE did not bring significant benefits, especially for plates that already had reasonable contrast. This result suggests that in environments with high variation in lighting, CLAHE may not be the best preprocessing option, as also discussed in the study "Design of IoT Based Automatic Number Plate Recognition" \cite{b10}, where lighting plays a critical role.
    
    In addition to accuracy, we also observed a slight drop in \textbf{precision}, which went from \textbf{71.7\%} without preprocessing to \textbf{70.75\%} with the use of CLAHE. Precision is a metric that measures the proportion of correct positive predictions. The drop suggests that the model with CLAHE produced more false positives or had difficulty correctly distinguishing characters under certain conditions, such as when plates were in adverse lighting conditions. Although CLAHE is a powerful tool for enhancing detail in low-contrast images, it can also introduce artifacts that confuse the OCR algorithm, especially in noisy images, as mentioned in "Deep Learning-Based Bangladeshi License Plate Recognition System" \cite{b11}.
    
    \textbf{Recall} — which measures the proportion of plates correctly detected relative to the total plates present — also showed a slight degradation, dropping from \textbf{71.7\%} (without preprocessing) to \textbf{70.75\%}. This implies that CLAHE did not increase the sensitivity of the OCR system to detect all the plates in the dataset, reinforcing the conclusion that contrast enhancement was not sufficient to compensate for the challenges inherent in image capture conditions.
    
    Consequently, the \textbf{F1-score}, which is the harmonic mean between precision and recall, also dropped, reaching \textbf{70.75\%}. As the F1-score is a metric that balances these two factors, its drop reflects the overall degradation in performance when applying CLAHE as preprocessing. This result is consistent with the literature, which highlights that CLAHE may not be ideal for all types of images, being more effective in scenarios where contrast is the main limitation, as shown in "A Framework for Automatic Detection of Traffic Violations" \cite{b12}.
    
    The \textbf{ROC Curve (Receiver Operating Characteristic)} and the \textbf{AUC (Area Under the Curve)} are important metrics for evaluating a model's ability to correctly classify instances, especially in binary classification problems. However, in our study, where each prediction refers to a single plate without a probabilistic model, these metrics remained unchanged. CLAHE did not impact the ROC curve or AUC, as shown in Figure~\ref{img10}, since the model configuration remained the same, and the analysis was based on a deterministic scenario. The ROC and AUC calculation showed identical results to the original model, which used EasyOCR without preprocessing, suggesting that these metrics did not capture significant improvements or deteriorations with CLAHE. As noted in "License Plate Recognition Method Based on Convolutional Neural Network" \cite{b14}, in scenarios where object detection such as plates is handled, variations in ROC and AUC may be more noticeable in systems with varying levels of uncertainty, which was not the case in this study.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img10.png}}
        \caption{The ROC Curve (Receiver Operating Characteristic) and AUC (Area Under the Curve) remained unchanged}
        \label{img10}
    \end{figure}
    
    Another important factor to consider is the \textbf{execution time}. When using only EasyOCR with the color images without preprocessing, the average execution time was \textbf{7.26 seconds}, with a median of \textbf{6.59 seconds}. However, when applying CLAHE as preprocessing, the average time increased to \textbf{8.87 seconds}, with a median of \textbf{7.13 seconds}, as shown in Figures~\ref{img11} and~\ref{img12}. This shows that CLAHE added a computational overhead to the process, increasing the model's execution time. Although the difference is not drastic, it suggests that using CLAHE should be carefully considered when processing time is a critical variable in real-time applications, such as real-time plate recognition systems on busy roads, as discussed in "Computer Vision Based Vehicle Detection for Toll Collection System Using Embedded Linux" \cite{b16}.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img11.png}}
        \caption{The arithmetic mean and median of the model's execution time}
        \label{img11}
    \end{figure}
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img12.png}}
        \caption{Gaussian Distribution of Execution Times}
        \label{img12}
    \end{figure}
    
    Comparing these results with those obtained when using grayscale, we note that CLAHE brought minimal changes in performance metrics and processing time. The expected contrast gain did not translate into a significant improvement in the OCR model's precision or recall, indicating that for this specific dataset, CLAHE did not offer real advantages. Similar results were observed in "StreetOCRCorrect: An Interactive Framework for OCR Corrections in Chaotic Indian Street Videos" \cite{b13}, where contrast adjustment techniques were also insufficient to improve character recognition in environments with extreme lighting variation.
    
    \textbf{CLAHE} has the advantage of improving contrast in low-quality images, being particularly useful in environments with lighting variations that hinder character visibility, such as vehicle plates under poor lighting or reflections. However, one of the \textbf{main weaknesses} is that this technique can introduce \textbf{visual artifacts} when applied to images that already have acceptable contrast or when there is too much noise, which can hinder OCR algorithms like EasyOCR. Additionally, the increase in processing time observed with CLAHE can also be a limiting factor in systems that require high efficiency.
    
    CLAHE is widely used in \textbf{medical imaging}, where enhancing details in dark areas is essential, and in \textbf{security image processing}, where visibility in low-light conditions is critical. However, in \textbf{vehicle license plate recognition systems}, the use of CLAHE should be carefully evaluated, especially in contexts where lighting conditions are not the limiting factor, or where there is significant variation in capture conditions, such as angles, distances, and outdoor environments.
    
    \subsection{Bilateral Filter - EasyOCR}
    
    The fourth preprocessing algorithm used in this study was the \textbf{bilateral filter}, aimed at verifying how the dataset would behave compared to using EasyOCR without any preprocessing. The bilateral filter is a non-linear smoothing technique that preserves image edges while reducing noise without eliminating important details. Unlike other smoothing filters, such as the mean or median filter, the bilateral filter is effective at smoothing homogeneous areas of the image while preserving edges, which can be useful for character recognition in noisy environments. However, our experiments indicated that applying the bilateral filter did not bring significant improvements.
    
    Specifically, \textbf{accuracy} — defined as the fraction of correct predictions out of the total predictions made — showed a slight drop, from \textbf{71.7\%} (without preprocessing) to \textbf{70.75\%} with the application of the bilateral filter. This result suggests that while the filter reduced noise in the images, it may have excessively smoothed some important areas, hindering EasyOCR's ability to read the characters. This performance degradation is consistent with observations made in studies such as "Automatic Vehicle Entry Control System" \cite{b9}, where excessive smoothing techniques resulted in the loss of critical information for character recognition.
    
    The model's \textbf{precision} was also affected, dropping from \textbf{71.7\%} to \textbf{70.75\%} after applying the bilateral filter. Precision measures the proportion of correct positive predictions out of all positive predictions made. This result indicates that the model was less effective in correctly identifying characters without generating false positives, which may be a consequence of smoothing important edges for character segmentation in plates. This effect may be exacerbated in scenarios where plates have fine details or sharp edges, such as in environments with damaged or dirty plates, as discussed in "A Hybrid Deep Learning Algorithm for License Plate Detection and Recognition in Vehicle-to-Vehicle Communications" \cite{b7}.
    
    \textbf{Recall} — which evaluates the proportion of correctly detected plates relative to the total plates in the dataset — was also reduced from \textbf{71.7\%} to \textbf{70.75\%}. This value indicates that the bilateral filter, despite being efficient at reducing noise in the images, did not improve OCR sensitivity to detect all plates in the dataset, especially under more adverse conditions, such as low lighting or occluded plates. This result reinforces the idea that the smoothing effect of the filter may have eliminated crucial details for character recognition in challenging scenarios.
    
    The \textbf{F1-score}, which is the harmonic mean between precision and recall, also dropped to \textbf{70.75\%}. The F1-score is particularly relevant in plate recognition contexts, where it is important to balance precision (correctly identifying characters) and recall (minimizing missed detections). This decrease reflects the bilateral filter's limitations in this specific dataset and corroborates the findings of previous research, such as "Recognition of Vehicle License Plates Using Various Segmentation Methods and OCR Techniques" \cite{b15}, which indicated that excessive smoothing can negatively impact plate recognition.
    
    \textbf{ROC Curve (Receiver Operating Characteristic)} and \textbf{AUC (Area Under the Curve)} remained unchanged from the original EasyOCR model, as shown in Figure~\ref{img13}. Since each prediction in our dataset refers to a single plate without probabilistic thresholds, the model's behavior remained unchanged with the application of the bilateral filter. This stability in ROC and AUC was expected, as in other studies involving deterministic OCR systems, such as "Real-Time License Plate Recognition Using YOLOv7 and OCR Technology" \cite{b17}, where similar metrics were observed in noise reduction scenarios.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img13.png}}
        \caption{The ROC Curve (Receiver Operating Characteristic) and AUC (Area Under the Curve) remained unchanged}
        \label{img13}
    \end{figure}
    
    Regarding \textbf{execution time}, the bilateral filter showed results comparable to other preprocessing techniques. When using EasyOCR without preprocessing, the average execution time was \textbf{7.26 seconds}, with a median of \textbf{6.59 seconds}. After applying the bilateral filter, the average time increased to \textbf{8.93 seconds}, with a median of \textbf{7.17 seconds}, as shown in Figures~\ref{img14} and~\ref{img15}. The increase in time is due to the additional computational complexity introduced by the bilateral filter, which needs to calculate both spatial and range filtering. Although this increase is not drastic, it suggests that the bilateral filter, like CLAHE, adds computational overhead to the process, especially in real-time plate recognition systems, where processing speed is critical.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img14.png}}
        \caption{The arithmetic mean and median of the model's execution time}
        \label{img14}
    \end{figure}
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img15.png}}
        \caption{Gaussian Distribution of Execution Times}
        \label{img15}
    \end{figure}
    
    The \textbf{Gaussian Distribution of Execution Times} in Figure~\ref{img15} shows that execution times followed a relatively normal distribution, indicating no significant outliers or processing anomalies. However, the slight increase in time compared to EasyOCR without preprocessing reinforces that the bilateral filter adds a noticeable, though not overwhelming, computational load to the process.
    
    \subsection{No Preprocessing - TesseractOCR}
    
    Finally, we applied TesseractOCR directly to the original images without any preprocessing to compare its performance with EasyOCR and other preprocessing techniques.
    
    TesseractOCR is one of the most widely used OCR libraries in academic and industrial research, and it has undergone significant improvements over the years. Developed by Hewlett-Packard in the 1980s and later maintained by Google, TesseractOCR is known for its robustness and flexibility. However, its performance depends significantly on the quality of the input images, as detailed in the study "Enhanced Vehicle License Plate Recognition Using TesseractOCR and Deep Learning Approaches" \cite{b18}.
    
    When applied to our dataset without any preprocessing, TesseractOCR presented an \textbf{accuracy} rate of approximately \textbf{66.03\%}, significantly lower than the \textbf{71.7\%} obtained with EasyOCR without preprocessing. This result suggests that TesseractOCR, although a widely used and reliable library, may not be as efficient as EasyOCR in reading vehicle plates without the application of image enhancement techniques. Additionally, we observed that TesseractOCR had more difficulty recognizing characters in noisy images or with poor lighting, confirming its sensitivity to environmental conditions, as discussed in "A Comparative Analysis of TesseractOCR and EasyOCR in Automatic License Plate Recognition Systems" \cite{b19}.
    
    \textbf{Precision}, which measures the proportion of correct positive predictions, also showed a significant drop, reaching \textbf{66.03\%} without preprocessing. This result highlights TesseractOCR's limitations in detecting characters in complex environments, such as plates with varying lighting and reflection conditions.
    
    Similarly, \textbf{recall}, which measures the proportion of correctly detected plates, was also \textbf{66.03\%}, indicating that TesseractOCR was less efficient at detecting plates in real-world conditions without preprocessing. This result contrasts with studies where TesseractOCR performed well in more controlled environments, such as "Real-Time Automatic License Plate Recognition Using TesseractOCR and YOLOv3" \cite{b20}, reinforcing the need for preprocessing in our specific dataset.
    
    Finally, the \textbf{F1-score} obtained with TesseractOCR was \textbf{66.03\%}, significantly lower than the \textbf{71.7\%} obtained with EasyOCR. This result reflects the overall degradation in TesseractOCR's performance compared to EasyOCR, confirming its limitations when dealing with noisy or low-quality images without preprocessing.
    
    The \textbf{ROC Curve} and \textbf{AUC} metrics were not significantly impacted by the change from EasyOCR to TesseractOCR, as expected, since both models worked deterministically with a single predicted plate. However, we observed that TesseractOCR's \textbf{execution time} was higher than that of EasyOCR, with an average of \textbf{10.2 seconds}, compared to \textbf{7.26 seconds} with EasyOCR, as shown in Figures~\ref{img16} and~\ref{img17}. This increase in execution time indicates that TesseractOCR, despite its flexibility, may require more computational resources for processing, making it less suitable for real-time applications.
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img16.png}}
        \caption{The arithmetic mean and median of the model's execution time}
        \label{img16}
    \end{figure}
    
    \begin{figure}[htbp]
        \centerline{\includegraphics[width=0.5\textwidth]{img17.png}}
        \caption{Gaussian Distribution of Execution Times}
        \label{img17}
    \end{figure}
    
    \section{Conclusion}
    
    This study demonstrated the application of different preprocessing techniques in the context of vehicle plate recognition using OCR, aiming to evaluate the impact of these techniques on model performance. The results showed that, in some cases, such as with grayscale conversion, there was no significant improvement in accuracy, precision, or recall, while in others, such as CLAHE and the bilateral filter, the application of preprocessing brought small improvements, but also added computational complexity.
    
    Additionally, the comparison between EasyOCR and TesseractOCR revealed that EasyOCR performed better in our dataset, both in terms of accuracy and processing speed. TesseractOCR, although widely used, was less efficient in our scenario, especially without the application of preprocessing techniques.
    
    In summary, this work highlights the importance of carefully choosing preprocessing techniques, considering not only their impact on model performance but also the computational cost they introduce. We hope that this research contributes to improving the development of vehicle plate recognition systems, with potential applications in traffic monitoring, public safety, and urban automation.
    
    \bibliographystyle{IEEEtran}
    \bibliography{references}
\end{document}
